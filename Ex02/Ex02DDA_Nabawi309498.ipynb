{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aeaa1b",
   "metadata": {},
   "source": [
    "### Mansoor Nabawi, #309498\n",
    "\n",
    "Tutorial 2, 05.13.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee5459",
   "metadata": {},
   "source": [
    "## Exercise 1: Data cleaning and text tokenization (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c64da",
   "metadata": {},
   "source": [
    "----------\n",
    "## $\\color{red}{\\text{Report 1}}$\n",
    "----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b50ab",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Flowchart of the processes}}$\n",
    "![diagram processes](exa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61e78a",
   "metadata": {},
   "source": [
    "#### Table of Times for 1\n",
    "| P=1    | P=2    | P=3    | P=4    | P=6 | P=8| P=10|\n",
    "|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|\n",
    "| 80.93    | 43.68   |   32.3  |   **25.73**  | 39.0 | 35.5 | 40 |\n",
    " \n",
    "\n",
    "- The result of parallelization is very obvious in this exercise.\n",
    "- As the number of workers increases, the computation time decreases until p=4.\n",
    "- our best result is when we use 4 workers.\n",
    "----------------\n",
    "*Strategy*\n",
    "\n",
    "\n",
    "- We loop over the files of each folder.\n",
    "- we share the files in each folder to different workers.\n",
    "- each worker will perform cleaning and tokenization on the subset of data it has and send the result back to rank0.\n",
    "- rank0 is our master process and it also work as worker, but at the end it receives all the data and add them in one big list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c1d4e",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{result of exercise 1}}$\n",
    "![title](res01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81f2894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DDA02_ex1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile DDA02_ex1.py\n",
    "\n",
    "#importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "#import nltk \n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "#loading manual stopwords\n",
    "from stop_words_man import stopw\n",
    "stopword_manually = stopw() #created stopwords manually\n",
    "\n",
    "\n",
    "#initialization\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "\n",
    "def clean_tokenized(file_n):\n",
    "    \"\"\"\n",
    "    This function receives a list of file names and goes through each file and removes\\\n",
    "    unnecessary words and symbols, it also checks every single word not to be of stopwords\\\n",
    "    at the end tokenize them by tputting them in a list.\n",
    "    args:\n",
    "        - file_n: file names\n",
    "    output:\n",
    "        - tokenized words (list)\n",
    "    \"\"\"\n",
    "    #empty lists will be used for final addtion and also filtering words.\n",
    "    tokenized_words = []\n",
    "    filtered_line = []\n",
    "    for file in (file_n):\n",
    "        with open(file, 'r', encoding = 'Latin-1') as f:\n",
    "            text = f.read()\n",
    "            #not word replaced by space\n",
    "        text = [re.sub(\"[^a-zA-Z]\", \" \", f) for f in text.split()]\n",
    "        text = \" \".join(text)#to remove white spaces\n",
    "        text = text.lower()\n",
    "        text = [f for f in text.split() if len(f)>1] #single words deleted\n",
    "        filtered_line = [w for w in text if not w in stopword_manually]\n",
    "        tokenized_words += [filtered_line] #tokenization\n",
    "        \n",
    "    #with open(\"/home/mansoor/Desktop/DDA/ex02/saved\"+f'/tokenized_rank_{rank}.ob', 'wb') as fp:\n",
    "    #    pickle.dump(tokenized_words, fp)\n",
    "    \n",
    "    return tokenized_words\n",
    "            \n",
    "                    \n",
    "    \n",
    "def main_function():\n",
    "    \"\"\"\n",
    "    This function is the main function to run the parallel program on.\n",
    "    \"\"\"\n",
    "    #time\n",
    "    t0 = MPI.Wtime()\n",
    "    \n",
    "    path = \"/home/mansoor/Desktop/DDA/ex02/20_newsgroups/\"\n",
    "    #folders in the path\n",
    "    folders = os.listdir(path)\n",
    "    #change the current working directory to path\n",
    "    os.chdir(path)\n",
    "\n",
    "    final_list = []\n",
    "    #going through each folders and files inside of them.\n",
    "    for i in range(len(folders)):\n",
    "        line_inner = []\n",
    "        filenames = []\n",
    "        inner_path = (path+str(folders[i]))\n",
    "        os.chdir(inner_path)\n",
    "        filenames += [f for f in os.listdir(inner_path) if os.path.isfile(os.path.join(inner_path,f))]\n",
    "        #determining the share for each process\n",
    "        share = round(len(filenames)/size)\n",
    "    \n",
    "\n",
    "        #only slaves\n",
    "        if rank != 0:\n",
    "\n",
    "            #receiving names of the files\n",
    "            filens = comm.recv(source=0, tag = 0)\n",
    "            #assigning files to ditexterent workers except worker 0\n",
    "\n",
    "            #cleaning and tokenization\n",
    "            tokenized = clean_tokenized(filens)\n",
    "\n",
    "\n",
    "            #sending to master node\n",
    "            comm.send(tokenized, dest=0, tag=1)\n",
    "            \n",
    "            \n",
    "        #master process\n",
    "        else:\n",
    "            \n",
    "            #distributing the data\n",
    "            for i in range(1, size):\n",
    "\n",
    "                comm.send(filenames[(share*i):(share*(1+i))], dest = i, tag=0)\n",
    "\n",
    "            #master node's share of file\n",
    "            filenames = filenames[(share*rank):(share*(1+rank))]\n",
    "            #cleaning and tokenization \n",
    "            tokenized = clean_tokenized(filenames)\n",
    "            #saving the file      \n",
    "            #with open(\"/home/mansoor/Desktop/DDA/ex02/saved\"+f'/tokenized_rank_{rank}.ob', 'wb') as fp:\n",
    "            #    pickle.dump(tokenized, fp)\n",
    "            #appending\n",
    "            line_inner.append(tokenized)\n",
    "\n",
    "            #receiving results\n",
    "            for i in range(1, size):\n",
    "            \n",
    "                tknzd = comm.recv(source = i, tag = 1)\n",
    "\n",
    "                line_inner.append(tknzd)\n",
    "\n",
    "        #appending final result\n",
    "        final_list.append(line_inner)\n",
    "        \n",
    "   \n",
    "    t1 = MPI.Wtime() - t0\n",
    "    print(\"Time: \", t1)\n",
    "    #print(len(final_list))\n",
    "    #save_csv(final_list, rank=10)\n",
    "    with open(\"/home/mansoor/Desktop/DDA/ex02/saved\"+f'/final_list_{size}.ob', 'wb') as fp:\n",
    "        pickle.dump(final_list, fp)\n",
    "    return final_list\n",
    "    \n",
    "main_func = main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59628061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  25.713739156723022\n",
      "Time:  25.72397494316101\n",
      "Time:  25.755807876586914\n",
      "Time:  25.765300989151\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python  DDA02_ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14d7a8",
   "metadata": {},
   "source": [
    "## Exercise02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ef96e",
   "metadata": {},
   "source": [
    "----------\n",
    "## $\\color{red}{\\text{Report 2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f297c",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Flowchart of the processes}}$\n",
    "![diagram processes](tf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc1fea",
   "metadata": {},
   "source": [
    "#### Table of Times for 2\n",
    "| P=2    | P=4    | P=6 | \n",
    "|:-----------|:-----------|:-----------|\n",
    "| 2.9  |   **2.3** | 6.02 |\n",
    " \n",
    "\n",
    "- The result of parallelization visible in this exercise\n",
    "- As the number of workers increases, the computation time decreases but only until 4.\n",
    "- our best result is when we use 4 workers.\n",
    "----------------\n",
    "*Strategy*\n",
    "\n",
    "\n",
    "- We loop over the files of each folder.\n",
    "- we share the files in each folder to different workers.\n",
    "- each worker will perform tf computation on the subset of data it has and send the result back to rank0.\n",
    "- rank0 is our master process and it also work as worker, but at the end it receives all the data and add them in one big list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11723b",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{result of exercise 2}}$\n",
    "![title](02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8482620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ex02_tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ex02_tf.py\n",
    "\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#using saved data\n",
    "with open ('saved/final_list_1.ob','rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "#initialization\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "#name = MPI.Get_processor_name()\n",
    "\n",
    "#batch = rank(len(data-1)/size)\n",
    "\n",
    "#tf calculator function\n",
    "def calculate_tf(tokenized_data):\n",
    "    tf_list = []\n",
    "    for document in tokenized_data:\n",
    "        sentence_dict = dict()\n",
    "        for word in document:\n",
    "            sentence_dict[word] = sentence_dict.get(word,0)+1\n",
    "        len_docu = len(document)\n",
    "        \n",
    "        for word in sentence_dict:\n",
    "            sentence_dict[word] = sentence_dict[word]/len_docu\n",
    "        tf_list.append(sentence_dict)\n",
    "    return tf_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t0 = MPI.Wtime()\n",
    "def main_function():\n",
    "    \"\"\"\n",
    "    This function is the main function to run the parallel program on.\n",
    "    \"\"\"\n",
    "    #time\n",
    "    \n",
    "    \n",
    "    final_list = []\n",
    "    #going through each folders and files inside of them.\n",
    "    for i in range(len(data)-1):\n",
    "        line_inner = []\n",
    "\n",
    "        #determining the share for each process\n",
    "        share = round(len(data[i][0])/size)\n",
    "        #the data is selected\n",
    "        selected = data[i][0]\n",
    "    \n",
    "\n",
    "        #only slaves\n",
    "        if rank != 0:\n",
    "\n",
    "            #receiving names of the files\n",
    "            filens = comm.recv(source=0, tag = 0)\n",
    "            #assigning files to ditexterent workers except worker 0\n",
    "            \n",
    "            #calculating tf\n",
    "            tf_res = calculate_tf(filens)\n",
    "\n",
    "            #sending to master node\n",
    "            comm.send(tf_res, dest=0, tag=1)\n",
    "            \n",
    "        \n",
    "        #master process\n",
    "        else:\n",
    "            \n",
    "            #distributing the data\n",
    "            for i in range(1, size):\n",
    "\n",
    "                comm.send(selected[(share*i):(share*(1+i))], dest = i, tag=0)\n",
    "\n",
    "            #master node's share of file\n",
    "            filens = selected[(share*rank):(share*(1+rank))]\n",
    "            #tf\n",
    "            tf_res = calculate_tf(filens)\n",
    "            line_inner.append(tf_res)\n",
    "\n",
    "            #receiving results\n",
    "            for i in range(1, size):\n",
    "            \n",
    "                tff = comm.recv(source = i, tag = 1)\n",
    "                line_inner.append(tff)\n",
    "                \n",
    "\n",
    "\n",
    "        #appending final result\n",
    "        final_list.append(line_inner)\n",
    "        \n",
    "   \n",
    "    t1 = MPI.Wtime() - t0\n",
    "    print(\"Time: \", t1)\n",
    "\n",
    "    with open(\"/home/mansoor/Desktop/DDA/ex02/saved\"+f'/tf_res_{size}.ob', 'wb') as fp:\n",
    "        pickle.dump(final_list, fp)\n",
    "    return final_list\n",
    "    \n",
    "main_func = main_function()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "315f0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  2.2453770637512207\n",
      "Time:  2.2325971126556396\n",
      "Time:  2.3546640872955322\n",
      "Time:  2.2729651927948\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python  ex02_tf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1d9b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just loading the data of tf\n",
    "import pickle\n",
    "\n",
    "#using saved data\n",
    "with open ('saved/final_list_1.ob','rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fd80d",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d06a3",
   "metadata": {},
   "source": [
    "----------\n",
    "## $\\color{red}{\\text{Report 3}}$\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736455de",
   "metadata": {},
   "source": [
    "#### Table of Times for 3\n",
    "| P=2    | P=4    | P=6 | \n",
    "|:-----------|:-----------|:-----------|\n",
    "| 7.9796  |   **5.9459**| 6.0164 |\n",
    " \n",
    "\n",
    "- The result of parallelization visible in this exercise\n",
    "- As the number of workers increases, the computation time decreases.\n",
    "- our best result is when we use 4 workers.\n",
    "----------------\n",
    "*Strategy*\n",
    "\n",
    "\n",
    "- first we use the tokenized data from previous exercise.\n",
    "- we append all of the documents in one list.\n",
    "- word_frequency_in_doc() function goes through the data find the occurence and then ratio of word in the each batch.\n",
    "- the result sent to the master process and it does the final frequency.\n",
    "- save the data at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6f8b7",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{result of exercise 3}}$\n",
    "![title](03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "551e94ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ex03_idf_0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ex03_idf_0.py\n",
    "\n",
    "#loading libraries\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "\n",
    "#using saved data\n",
    "with open ('saved/final_list_1.ob','rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "    total_docs = []\n",
    "    for i in range(len(data)-1):\n",
    "        for j in range(len(data[i][0])):\n",
    "            total_docs.append(data[i][0][j])\n",
    "\n",
    "\n",
    "def word_frequency_in_doc(data):\n",
    "    word_folder = dict()\n",
    "    for i in range(len(data)):\n",
    "        # Get unique words per document\n",
    "        words = np.unique(data[i])\n",
    "        # Counting the times a word has been mentioned in a document\n",
    "        for word in words:\n",
    "            if word in word_folder:\n",
    "                word_folder[word] += 1\n",
    "            else:\n",
    "                word_folder[word] = 1\n",
    "    batch = {k: (len(data) / v) for k, v in word_folder.items()}\n",
    "    return batch\n",
    "\n",
    "\n",
    "p_ranks = round(len(total_docs)/(size-1))\n",
    "\n",
    "\n",
    "total = dict()\n",
    "\n",
    "#master process\n",
    "if rank == 0:\n",
    "    t0 = MPI.Wtime()\n",
    "    total_docs2 = total_docs[0:p_ranks]\n",
    "    for i in range(1, size):\n",
    "        total_docs1 = total_docs[(i*p_ranks):(p_ranks*(i+1))]\n",
    "        comm.send(total_docs1, dest=i)\n",
    "    output1 = word_frequency_in_doc(total_docs2)\n",
    "\n",
    "    global_dict = None\n",
    "    for i in range(1, size):\n",
    "        idf = comm.recv()\n",
    "        output1 = (Counter(output1) + Counter(idf))\n",
    "    total = {k: np.log(v / (size)) for k, v in output1.items()}\n",
    "   \n",
    "    print('Time:',MPI.Wtime() - t0)\n",
    "    with open(\"/home/mansoor/Desktop/DDA/ex02/saved\"+f'/idf_res_{size}.ob', 'wb') as fp:\n",
    "        pickle.dump(total, fp)\n",
    "            \n",
    "#slaves\n",
    "else:\n",
    "    data = comm.recv()\n",
    "    output = word_frequency_in_doc(data)\n",
    "    comm.send(output, dest=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3e9ee46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5.945959806442261\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python ex03_idf_0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a4119105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using saved data\n",
    "with open ('saved/idf_res_4.ob','rb') as fp:\n",
    "    idf_ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c3cc4",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c086e45",
   "metadata": {},
   "source": [
    "----------\n",
    "## $\\color{red}{\\text{Report 4}}$\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c8431",
   "metadata": {},
   "source": [
    "#### Table of Times for 4\n",
    "| P=2    | P=4    | P=6 | \n",
    "|:-----------|:-----------|:-----------|\n",
    "| 15.98  |   **12.679**| 17.332 |\n",
    " \n",
    "\n",
    "- The result of parallelization visible in this exercise\n",
    "- As the number of workers increases, the computation time decreases.\n",
    "- our best result is when we use 4 workers.\n",
    "----------------\n",
    "*Strategy*\n",
    "\n",
    "\n",
    "- first we use the tokenized data from previous exercise.\n",
    "- we append all of the documents in one list.\n",
    "- the master process sends batches of data to the remaining workers.\n",
    "\n",
    "- each worker will accomplish 2 tasks: first get the TF for each document and secondly it will get the IDF for all the documents that each has received from the root.\n",
    "- Master will merge the data from all the processes, measure the final IDF and multiply with each token TF.\n",
    "- save the data at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3044d1",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{result of exercise 4}}$\n",
    "![title](04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ddc1c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ex04_tf_idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ex04_tf_idf.py\n",
    "\n",
    "#loading libraries\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "\n",
    "\n",
    "#using saved data\n",
    "with open ('saved/final_list_1.ob','rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "    total_docs = []\n",
    "    for i in range(len(data)-1):\n",
    "        for j in range(len(data[i][0])):\n",
    "            total_docs.append(data[i][0][j])\n",
    "\n",
    "\n",
    "\n",
    "def calculate_tfidf(data):\n",
    "    chunk = []\n",
    "    word_folder = dict()\n",
    "    for i in range(len(data)):\n",
    "        word = defaultdict(int)\n",
    "        unique_tf = np.unique(data[i])\n",
    "        counter = len(np.unique(unique_tf))\n",
    "        for i in data[i]:\n",
    "            word[i] +=1\n",
    "        dictionary = {k: v / counter for k, v in word.items()}\n",
    "        chunk.append(dictionary)\n",
    "        for unique in unique_tf:\n",
    "            if unique in word_folder:\n",
    "                word_folder[unique] += 1\n",
    "            else:\n",
    "                word_folder[unique] = 1\n",
    "    batch = {k: (len(data) / v) for k, v in word_folder.items()}\n",
    "\n",
    "    return chunk, batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p_ranks = round(len(total_docs)/(size-1))\n",
    "final_idf = dict()\n",
    "final = []\n",
    "dictcalculate_tfidf = dict()\n",
    "#master process\n",
    "if rank == 0:\n",
    "    t0 = MPI.Wtime()\n",
    "    total_docs2 = total_docs[0:p_ranks]\n",
    "    for i in range(1, size):\n",
    "        total_docs1 = total_docs[(i*p_ranks):(p_ranks*(i+1))]\n",
    "        comm.send(total_docs1, dest=i)\n",
    "    TF1, IDF1 = calculate_tfidf(total_docs2)\n",
    "\n",
    "    global_dict = None\n",
    "    for i in range(1, size):\n",
    "        chunka = []\n",
    "        TF2, IDF2 = comm.recv()\n",
    "        TF1 = TF1 + TF2\n",
    "        IDF1 = (Counter(IDF2) + Counter(IDF1))\n",
    "    final = TF1 + final\n",
    "    final_idf = {k: np.log(v / (size-1)) for k, v in IDF1.items()}\n",
    "    for i in range(len(final)):\n",
    "        n = final[i]\n",
    "        for key, value in n.items():\n",
    "            if key in final_idf:\n",
    "                dictcalculate_tfidf[key] = (n[key]) * (final_idf[key])\n",
    "            else:\n",
    "                None \n",
    "                \n",
    "    print('Time:',MPI.Wtime() - t0)\n",
    "\n",
    "else:\n",
    "    data = comm.recv()\n",
    "    TF, IDF = calculate_tfidf(data)\n",
    "    output = (TF, IDF)\n",
    "    comm.send(output, dest=0)\n",
    "\n",
    "\n",
    "with open(\"/home/mansoor/Desktop/DDA/ex02/saved\"+f'/tfidf_res_{size}.ob', 'wb') as fp:\n",
    "        pickle.dump(dictcalculate_tfidf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f427ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12.67906403541565\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python ex04_tf_idf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "53bb92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using saved data\n",
    "with open ('saved/tfidf_res_4.ob','rb') as fp:\n",
    "    tfidf_ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63a7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
